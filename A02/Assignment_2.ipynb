{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fx67bo_ImD6-",
        "outputId": "fc9b71c8-3e0b-4881-cd9b-52788a3b86d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GPT-2 model and tokenizer...\n",
            "Model and tokenizer loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import re # Import regular expression module\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "print(\"Loading GPT-2 model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "def get_seventh_highest_probable_word(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates the seventh highest probable next word for a given prompt\n",
        "    using the GPT-2 model, ensuring only actual words are considered.\n",
        "    \"\"\"\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Get model predictions (logits) for the next token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the logits for the last token in the sequence\n",
        "    last_token_logits = logits[0, -1, :]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Get the sorted probabilities and their indices\n",
        "    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "\n",
        "    seventh_word_found = False\n",
        "    word_count = 0\n",
        "    seventh_highest_word = \"\"\n",
        "\n",
        "    for token_id in sorted_indices:\n",
        "        decoded_word = tokenizer.decode(token_id.item()).strip()\n",
        "\n",
        "        # Filter out tokens that are not purely alphabetic (i.e., not 'words')\n",
        "        if decoded_word.isalpha(): # Use isalpha() for stricter 'word' definition\n",
        "            word_count += 1\n",
        "            if word_count == 7:\n",
        "                seventh_highest_word = decoded_word\n",
        "                seventh_word_found = True\n",
        "                break\n",
        "\n",
        "    if not seventh_word_found:\n",
        "        return \"Error: Not enough actual words to determine the 7th highest.\"\n",
        "\n",
        "    return seventh_highest_word\n",
        "\n",
        "# Example usage:\n",
        "# prompt_sentence = \"The quick brown fox jumps over the\"\n",
        "# predicted_word = get_seventh_highest_probable_word(prompt_sentence)\n",
        "# print(f\"Prompt: '{prompt_sentence}'\")\n",
        "# print(f\"Seventh highest probable word: '{predicted_word}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fceb1a0"
      },
      "source": [
        "Please provide your poem in the `poem_lines` variable in the next cell. The code will then process each line as described."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42988267",
        "outputId": "ac82cf50-1750-4a55-b6e1-94965e90daae"
      },
      "source": [
        "# Example poem. Replace this with your own poem.\n",
        "poem_text = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "# Split the multiline string into a list of lines\n",
        "poem_lines = poem_text.strip().splitlines()\n",
        "\n",
        "modified_poem_lines = []\n",
        "\n",
        "for line in poem_lines:\n",
        "    # Split the line into words\n",
        "    words = line.split()\n",
        "\n",
        "    if len(words) > 1:\n",
        "        # Form the prompt by taking all words except the last one\n",
        "        prompt_for_line = ' '.join(words[:-1])\n",
        "        predicted_word = get_seventh_highest_probable_word(prompt_for_line)\n",
        "        # Reconstruct the line with the predicted word\n",
        "        modified_line = f\"{prompt_for_line} {predicted_word}\"\n",
        "    elif len(words) == 1:\n",
        "        # If there's only one word, the prompt is that word itself\n",
        "        prompt_for_line = words[0]\n",
        "        predicted_word = get_seventh_highest_probable_word(prompt_for_line)\n",
        "        modified_line = f\"{prompt_for_line} {predicted_word}\"\n",
        "    else:\n",
        "        # Handle empty lines\n",
        "        modified_line = \"\"\n",
        "\n",
        "    modified_poem_lines.append(modified_line)\n",
        "\n",
        "print(\"Original Poem:\")\n",
        "for line in poem_lines:\n",
        "    print(line)\n",
        "\n",
        "print(\"\\nModified Poem (Seventh highest probable word):\")\n",
        "for line in modified_poem_lines:\n",
        "    print(line)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Poem:\n",
            "One must have a mind of winter\n",
            "To regard the frost and the boughs\n",
            "Of the pine-trees crusted with snow;\n",
            "And have been cold a long time\n",
            "To behold the junipers shagged with ice,\n",
            "The spruces rough in the distant glitter\n",
            "Of the January sun; and not to think\n",
            "Of any misery in the sound of the wind,\n",
            "In the sound of a few leaves,\n",
            "Which is the sound of the land\n",
            "Full of the same wind\n",
            "That is blowing in the same bare place\n",
            "For the listener, who listens in the snow,\n",
            "And, nothing himself, beholds\n",
            "Nothing that is not there and the nothing that is.\n",
            "\n",
            "Modified Poem (Seventh highest probable word):\n",
            "One must have a mind of her\n",
            "To regard the frost and the death\n",
            "Of the pine-trees crusted with oil\n",
            "And have been cold a long few\n",
            "To behold the junipers shagged with white\n",
            "The spruces rough in the distant night\n",
            "Of the January sun; and not to have\n",
            "Of any misery in the sound of the sound\n",
            "In the sound of a few shots\n",
            "Which is the sound of the voice\n",
            "Full of the same story\n",
            "That is blowing in the same bare footed\n",
            "For the listener, who listens in the following\n",
            "And, nothing himself, I\n",
            "Nothing that is not there and the nothing that isn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fc3fdd9"
      },
      "source": [
        "### Customizable Nth Probable Word Poem Analysis\n",
        "\n",
        "Below, you can define the `word_rank` to choose which probable word (e.g., 1 for highest, 7 for seventh, etc.) will be used to modify your poem. The code will then process each line of the `poem_text` using this specified rank."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990,
          "referenced_widgets": [
            "5d905bc96e8244919029b22aead2923d",
            "c829da626cea47c09c95df8bbb0a4f2d",
            "dd6cebed75ae48e2b5e652e478768101",
            "00cf8d9a5e6348ef8bd826cca6205d8e",
            "01bf64d620254ea9b0720b8f55b9c058",
            "5ef19bb625e34be6b0da8aa761770fa6",
            "64eabae45a1344a0830b92c8635ca251",
            "614c425480734fb49d86cfe3bfb4b723",
            "042c3fd98a1d4eac98acf8e6ae585553",
            "ebd1b6186c914a05bd4fbdc9fab76545",
            "4eab9070c546434886ff600bf3446d58",
            "cfd13582f75b40bdab1c25755f478ba2",
            "769f9bd38d5f4c50b9a3afec0e4fba66",
            "b0bf31b823544cabba5e16755003c0b1",
            "12a84496f66b4c78830e5044ed7a23a5",
            "aeb505a6ddaf41ca9cddba0dac6d4779",
            "809687f119854e6f88b0b48cfe53519d",
            "8a7e2d37db0240c6a720a305580110ee",
            "1a64934946564d39b7a3fb482fb00570",
            "f7423f20e48b40edb8770e9efb639ce5",
            "6506d3fc3f604bf9bf9ac2bfe663b7b2",
            "03e3da1639ad42e5a41428c7d707ef0e",
            "a4b79c36a4694a5c861e1f7fcb0f3015",
            "4ccafd23c6db460ab6c234d4b232f4e8",
            "b7f668863a6b40ecad1599cf49f127d9",
            "e87b2599e41043e99cbf8784d53dba4c",
            "96d9e0bcd5c64c3c9e7a1c45d0e56fb1",
            "20a4e7417d634647a9e8335ae9c18ae1",
            "4f001f1465254879a72dbd25e4f7154e",
            "d49e642770724bee832057ea1c7622ca",
            "ed62f59afc5d446ba80c3fb0f2f8040e",
            "5791b4ad613f4338be26ff775b16cb60",
            "01780432a893485d843cc4d134fd6342",
            "8391042a316349c581fe94ad2e242888",
            "a299c6ca746f4079a86696a20a677178",
            "ef788ad126fa412bb1457071df137898",
            "a9edbef9cd604ba8b3a5495f7a13b018",
            "cec2156c387141c19659ae8b205e7f7d",
            "df8b67e8e65147be9c7fe9ed2b92777a",
            "4f17dc4126784c7886fb9fdd4f8bbf3d",
            "b318012f2e81457f8649a5b538f9d3e4",
            "062d38094d1f46a896760eec736f8a90",
            "970a1ed990a24db287cc02716f39c56c",
            "4be38d3aa7224a999a8501c56c8146c1",
            "e542b94758bf47f69d94c5950f0dd11c",
            "4ccbeafc341247a59770e19f9d39cabe",
            "5836f950f8f64c7181acfc5e6778cce4",
            "5f3054dd497347d4a4d1743fe4e02aea",
            "5e2bd48824354c57bd3fecc59ea2ef34",
            "e7661d973f364438b80add035fcf0810",
            "b517014d497c4a68bc4b47320491e79e",
            "4983e94bdc654294b59fa9536bcbd850",
            "5e83b0f7900642d6b4d81ceb3b8d0d01",
            "a113fc264af9462ebe0c6d2c46293dbc",
            "8ea86c2648d14f698af6a834d62eb2ef",
            "ad8ca5555ed346a494b2e471fc14a1be",
            "899674798d80440dbabaff9cfca6c6bd",
            "eeaa7f63f80a463f93dedc7a281c1f30",
            "e29ed86cfa204d1ca33cd8baa1cc745c",
            "bd95fd479c904826ba5d4b79cae06749",
            "b2c4b100435a49febeda1ff0c4e2db50",
            "e8dd9205a7284dab8d69a423cedb09b0",
            "c747c8862dcd4e88b7186452edb4580a",
            "4c8ebcf03d2148d998022de22b1b5bdc",
            "68b7cca7b09647b5af774e282e8aaf02",
            "9692ce2238514763933083345971a11c",
            "cf6314be6fe64b60affefee9415110cb",
            "d3d36c6e4faa42f6b3a98dd17386c417",
            "b446c22971b94e8a8dabbae799790d81",
            "8d949e323ed84803bf8ac58259756ab6",
            "640af0cd08294edd81d69c1564ecba83",
            "e7bd8c75bdbc4ca78f62d2983ebc3c75",
            "d7fd34e6b2124019a16a3d909337a9e9",
            "33f1838ad93a42f58910819b8f600e34",
            "f41f355f9bf744e29af63e84a24b50c8",
            "22365ffc3d044e3d87ce7338e70856f9",
            "4c74fb51dc224425896e55309fc99243"
          ]
        },
        "id": "201bae1d",
        "outputId": "3c4dc63e-12c7-4b55-9d92-73f707a891cc"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import re # Import regular expression module\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer (repeated for self-contained cell)\n",
        "print(\"Loading GPT-2 model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "print(\"Model and tokenizer loaded successfully.\")\n",
        "\n",
        "# Define the rank of the probable word you want to use (e.g., 1 for highest, 7 for seventh)\n",
        "word_rank = 666 # You can change this number!\n",
        "\n",
        "def get_nth_highest_probable_word(prompt: str, n: int) -> str:\n",
        "    \"\"\"\n",
        "    Generates the nth highest probable next word for a given prompt\n",
        "    using the GPT-2 model, ensuring only actual words are considered.\n",
        "    \"\"\"\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Get model predictions (logits) for the last token in the sequence\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "    last_token_logits = logits[0, -1, :]\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # Get the sorted probabilities and their indices\n",
        "    sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)\n",
        "\n",
        "    nth_word_found = False\n",
        "    word_count = 0\n",
        "    nth_highest_word = \"\"\n",
        "\n",
        "    for token_id in sorted_indices:\n",
        "        decoded_word = tokenizer.decode(token_id.item()).strip()\n",
        "\n",
        "        # Filter out tokens that are not purely alphabetic (i.e., not 'words')\n",
        "        if decoded_word.isalpha():\n",
        "            word_count += 1\n",
        "            if word_count == n:\n",
        "                nth_highest_word = decoded_word\n",
        "                nth_word_found = True\n",
        "                break\n",
        "\n",
        "    if not nth_word_found:\n",
        "        return f\"Error: Not enough actual words to determine the {n}th highest.\"\n",
        "\n",
        "    return nth_highest_word\n",
        "\n",
        "# Your poem as a multiline string\n",
        "poem_text = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "# Split the multiline string into a list of lines\n",
        "poem_lines = poem_text.strip().splitlines()\n",
        "\n",
        "modified_poem_lines_nth = []\n",
        "\n",
        "for line in poem_lines:\n",
        "    # Split the line into words\n",
        "    words = line.split()\n",
        "\n",
        "    if len(words) > 1:\n",
        "        # Form the prompt by taking all words except the last one\n",
        "        prompt_for_line = ' '.join(words[:-1])\n",
        "        predicted_word = get_nth_highest_probable_word(prompt_for_line, word_rank)\n",
        "        # Reconstruct the line with the predicted word\n",
        "        modified_line = f\"{prompt_for_line} {predicted_word}\"\n",
        "    elif len(words) == 1:\n",
        "        # If there's only one word, the prompt is that word itself\n",
        "        prompt_for_line = words[0]\n",
        "        predicted_word = get_nth_highest_probable_word(prompt_for_line, word_rank)\n",
        "        modified_line = f\"{prompt_for_line} {predicted_word}\"\n",
        "    else:\n",
        "        # Handle empty lines\n",
        "        modified_line = \"\"\n",
        "\n",
        "    modified_poem_lines_nth.append(modified_line)\n",
        "\n",
        "print(\"Original Poem:\")\n",
        "for line in poem_lines:\n",
        "    print(line)\n",
        "\n",
        "print(f\"\\nModified Poem ({word_rank}th highest probable word):\")\n",
        "for line in modified_poem_lines_nth:\n",
        "    print(line)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GPT-2 model and tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d905bc96e8244919029b22aead2923d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cfd13582f75b40bdab1c25755f478ba2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b79c36a4694a5c861e1f7fcb0f3015"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8391042a316349c581fe94ad2e242888"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e542b94758bf47f69d94c5950f0dd11c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad8ca5555ed346a494b2e471fc14a1be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf6314be6fe64b60affefee9415110cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n",
            "Original Poem:\n",
            "One must have a mind of winter\n",
            "To regard the frost and the boughs\n",
            "Of the pine-trees crusted with snow;\n",
            "And have been cold a long time\n",
            "To behold the junipers shagged with ice,\n",
            "The spruces rough in the distant glitter\n",
            "Of the January sun; and not to think\n",
            "Of any misery in the sound of the wind,\n",
            "In the sound of a few leaves,\n",
            "Which is the sound of the land\n",
            "Full of the same wind\n",
            "That is blowing in the same bare place\n",
            "For the listener, who listens in the snow,\n",
            "And, nothing himself, beholds\n",
            "Nothing that is not there and the nothing that is.\n",
            "\n",
            "Modified Poem (666th highest probable word):\n",
            "One must have a mind of considerable\n",
            "To regard the frost and the cessation\n",
            "Of the pine-trees crusted with muc\n",
            "And have been cold a long bus\n",
            "To behold the junipers shagged with fright\n",
            "The spruces rough in the distant lunar\n",
            "Of the January sun; and not to join\n",
            "Of any misery in the sound of the coach\n",
            "In the sound of a few I\n",
            "Which is the sound of the French\n",
            "Full of the same fruit\n",
            "That is blowing in the same bare post\n",
            "For the listener, who listens in the email\n",
            "And, nothing himself, Smith\n",
            "Nothing that is not there and the nothing that ev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ad7b887",
        "outputId": "74c0bd5b-bbfa-4b10-f1b8-3af8b759b8a4"
      },
      "source": [
        "prompt_sentence = \"One must have a mind of\"\n",
        "predicted_word = get_seventh_highest_probable_word(prompt_sentence)\n",
        "print(f\"Prompt: '{prompt_sentence}'\")\n",
        "print(f\"Seventh highest probable word: '{predicted_word}'\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'One must have a mind of'\n",
            "Seventh highest probable word: 'her'\n"
          ]
        }
      ]
    }
  ]
}